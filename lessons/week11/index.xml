<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Week 11: GPU Programming on PSU Astro 528</title><link>https://psuastro528.github.io/lessons/week11/</link><description>Recent content in Week 11: GPU Programming on PSU Astro 528</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://psuastro528.github.io/lessons/week11/index.xml" rel="self" type="application/rss+xml"/><item><title>Exercises</title><link>https://psuastro528.github.io/lessons/week11/lab/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://psuastro528.github.io/lessons/week11/lab/</guid><description>Lab 8: Parallel Programming III: Hardware Accelerators &amp;amp; GPUs (due Nov 11)
Exercise 1 TBD</description></item><item><title>Learning goals</title><link>https://psuastro528.github.io/lessons/week11/goals/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://psuastro528.github.io/lessons/week11/goals/</guid><description>Week 11 Lab 8, Exercise 1 Run GPU code on ICDS-ACI Accelerate linear algebra computations with GPU Recognize what problem sizes and likely to result in acceleration with a GPU for linear algebra Lab 8, Exercise 2: Perform custom scientific computations using high-level GPU interface, such as Folds.jl with CUDAEx() executor from FoldsCUDA.jl (recommended), or mapreduce on CuArray from CUDA.jl (recommended) Improve performance by reducing kernel launches via broadcasting and GPU kernel fusion Improve performance by reducing memory transfers via GPU reductions Recognize what types of problems and problem sizes are likely to result in acceleration with a GPU when using a high-level programming interface Lab 8, Exercise 3: Write a GPU kernel, using one of CUDA.</description></item></channel></rss>