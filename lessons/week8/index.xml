<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Week 8: Parallel Programming I on PSU Astro 528</title><link>https://psuastro528.github.io/lessons/week8/</link><description>Recent content in Week 8: Parallel Programming I on PSU Astro 528</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="https://psuastro528.github.io/lessons/week8/index.xml" rel="self" type="application/rss+xml"/><item><title>Exercises</title><link>https://psuastro528.github.io/lessons/week8/lab/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://psuastro528.github.io/lessons/week8/lab/</guid><description> Lab 6: Parallel Programming I: Shared-memory Systems (due Oct 21)
Exercise 1: Parallelization for Multi-Core Workstations via Multiple-Threads Exercise 2: Parallelization for Multi-Core Workstations via Multiple Processes</description></item><item><title>Learning goals</title><link>https://psuastro528.github.io/lessons/week8/goals/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://psuastro528.github.io/lessons/week8/goals/</guid><description>Week 8 Lab 6: Shared Memory Computing Patterns Choose an appropriate number of worker processors for your compute node &amp;amp; problem
Parallelize code using shared memory model &amp;amp; multiple threads, using tools such as:
ThreadsX.map and ThreadsX.mapreduce Threads.@threads ThreadsX.foreach FLoops.jl and ThreadedEx (recommended) Threads Folds.jl (alternative) Parallelize code using shared memory model &amp;amp; multiple processes, using tools such as:
pmap SharedArrays DistributedArrays.</description></item><item><title>Readings</title><link>https://psuastro528.github.io/lessons/week8/read/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://psuastro528.github.io/lessons/week8/read/</guid><description>Week 8 Prior to Monday&amp;rsquo;s class Introduction to Parallel Computing Sec B-E -or- Introduction to High-Performance Computing Ch 2.1-2.4: Parallel Computing (19pg) -or- Introduction to High Performance Computing for Scientists and Engineers: Ch 4 Parallel Computers (20pg) -and- Ch 5 Basics of Parallelization (22pg) Lessons / Resources OpenMP (for students with projects using C/C++ or Fortran) Parallel Programming in OpenMP -or- Introduction to High Performance Computing for Scientists and Engineers Ch 6: Shared-memory Parallel Programming w/ OpenMP (22pg) Introduction to High Performance Computing for Scientists and Engineers Ch 7.</description></item></channel></rss>